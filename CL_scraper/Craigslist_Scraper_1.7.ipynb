{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Craigslist Apartment Scraper\n",
    "\n",
    "The purpose of this script is to pull apartment listings, characteristics, prices, and reply emails from for rent ads on craigslist. We plan to use this information in order to run an experiment to test the impact of including exclamation points on response rates to inquiries sent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version Notes  \n",
    "- This version does not include any testing code cells. For testing code cells, refer to v1.5\n",
    "- Unless you wish to pull new listings, be careful not to run the cell that runs queries in all four cities for the bedroom and price combinations specified\n",
    "- This version pulls listings from Seattle, Houston, Atlanta, and Chicago"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Import modules\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs4\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import xlsxwriter\n",
    "import time\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Set directory for input / output\n",
    "\n",
    "#DIRECTORY FOR HOME MACHINE\n",
    "os.chdir('/Users/nwchen24/Desktop/UC_Berkeley/Experiments_and_causality/final_project_github_repo/mids-w241-final/CL_scraper/output')\n",
    "\n",
    "#DIRECTORY FOR WORK MACHINE\n",
    "#os.chdir('C:/Users/nchen/Desktop/Nick/UC_Berkeley/experiments_and_causality/final_project_github_repo/mids-w241-final/CL_scraper/output')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to query craigslist  \n",
    "\n",
    "This function will allow us to specify a price range, the number of bedrooms, and what craigslist site to query (e.g. Denver, SF, NYC, etc.)  \n",
    "\n",
    "Note that these queries only return a max of 100 results each. Thus, we will want to be specific about the price ranges and bedrooms that we specify so we can maximize the number of listings we are able to capture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define a function to fetch search results\n",
    "def fetch_search_results(query=None, minAsk=None, maxAsk=None, bedrooms=None, search_URL = None):\n",
    "    search_params = {key: val for key, val in locals().items() if val is not None}\n",
    "    if not search_params:\n",
    "        raise ValueError(\"No valid keywords\")\n",
    "    resp = requests.get(search_URL, params=search_params, timeout=3)\n",
    "    resp.raise_for_status()  # <- no-op if status==200\n",
    "    return resp.content, resp.encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to get full URLs and apartment characteristics from query function output  \n",
    "\n",
    "This function will go through each of the listings found from our query and compile a dataset of URLs and apartment characteristics of all the results from the query. We will use the URLs to get the reply email addresses in a later step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions to get apartment characteristics from each query result  \n",
    "\n",
    "price, bedrooms, square footage, listing title, posting date / time, and reply linnk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get href - the relative link to the full apartment listing. These relative links are identified by <a> tags\n",
    "#and have the class 'result-title hdrlnk'.\n",
    "def get_href(result):\n",
    "    href = result.find('a', {'class' : 'result-title hdrlnk'})['href']\n",
    "    \n",
    "    if href is None:\n",
    "        href = np.nan\n",
    "    \n",
    "    return href"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get posting ID - These IDs are the data-ID portion of  <a> tags with the class 'result-title hdrlnk'.\n",
    "def get_posting_ID(result):\n",
    "    posting_ID = result.find('a', {'class' : 'result-title hdrlnk'})['data-id']\n",
    "    \n",
    "    if posting_ID is None:\n",
    "        posting_ID = np.nan\n",
    "    \n",
    "    return posting_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get price - price can be located by <span> tags of class 'result-price'\n",
    "def get_price(result):\n",
    "    price = result.find('span', {'class' : 'result-price'})\n",
    "    \n",
    "    #convert price to float\n",
    "    if price is not None:\n",
    "        price = float(price.text.strip('$'))\n",
    "        \n",
    "    else:\n",
    "        price = np.nan\n",
    "    \n",
    "    return price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get listing title which is identified by the text in the <a> tag with class 'result-title hdrlnk'\n",
    "def get_title(result):\n",
    "    title = result.find('a', {'class' : 'result-title hdrlnk'}).text\n",
    "    \n",
    "    if title is None:\n",
    "        title = np.nan\n",
    "        \n",
    "    return title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get the time the listing was posted\n",
    "def get_posting_date(result):\n",
    "    posting_date = result.find('time', {'class' : 'result-date'})['datetime']\n",
    "    \n",
    "    if posting_date is None:\n",
    "        posting_date = np.nan\n",
    "        \n",
    "    return posting_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get bedrooms / sqft which is identified by the <span> tag of class 'housing'\n",
    "def get_bedrooms_sqft_str(result):\n",
    "    bedrooms_sqft = result.find('span', {'class' : 'housing'}).text.strip('\\n')\n",
    "    \n",
    "    if bedrooms_sqft is None:\n",
    "        price = np.nan\n",
    "    \n",
    "    return bedrooms_sqft\n",
    "\n",
    "def get_bedrooms_sqft(bedrooms_sqft):\n",
    "    #*******\n",
    "    #remove the new line characters and white space\n",
    "    p_1 = re.compile('-|\\n|\\s')\n",
    "\n",
    "    bedrooms_sqft = p_1.sub('', bedrooms_sqft)\n",
    "\n",
    "    #*******\n",
    "    #get bedrooms\n",
    "    #compile the regex\n",
    "    bedroom_p = re.compile(r'\\d+(?=br)', re.IGNORECASE)\n",
    "\n",
    "    #get match in the bedroom / sqft string\n",
    "    bedroom_m = bedroom_p.match(bedrooms_sqft)\n",
    "\n",
    "    #get bedrooms\n",
    "    n_bedrooms = float(bedrooms_sqft[bedroom_m.start(): bedroom_m.end()])\n",
    "\n",
    "    #*******\n",
    "    #get square footage\n",
    "    #remove bedrooms\n",
    "    bedrooms_sqft = bedrooms_sqft[bedroom_m.end() + 2:]\n",
    "\n",
    "    #compile the regex\n",
    "    sqft_p = re.compile(r'\\d+(?=ft)', re.IGNORECASE)\n",
    "\n",
    "    #get match in the square footage string\n",
    "    sqft_m = sqft_p.match(bedrooms_sqft)\n",
    "\n",
    "    #get square footage\n",
    "    try:\n",
    "        sqft = float(bedrooms_sqft[sqft_m.start():sqft_m.end()])\n",
    "    \n",
    "    except AttributeError:\n",
    "        sqft = np.nan\n",
    "    \n",
    "    return n_bedrooms, sqft\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to compile all apartment characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compile_listing_URLs(query_result, base_URL, reply_string, city):\n",
    "    #parse the results of the query\n",
    "    html = bs4(query_result, 'html.parser')\n",
    "\n",
    "    #get all individual apartments from the query\n",
    "    apt_results = html.find_all('p', attrs={'class' : 'result-info'})\n",
    "\n",
    "    #initialize a list to contain all of the URLs that resulted from the query\n",
    "    apts_results_df = pd.DataFrame(columns = ('city', 'base_URL', 'href','posting_ID', 'Listing_Title', 'Bedrooms', 'Sqft', 'Price', 'Posting_Date'))\n",
    "   \n",
    "    #Looop through all of the tags containing the apartments and get the addresses of those individual results.\n",
    "    for apt in range(len(apt_results)):\n",
    "        #use helper functions to get characteristics\n",
    "        href = get_href(apt_results[apt])\n",
    "        posting_ID = get_posting_ID(apt_results[apt])\n",
    "        title = get_title(apt_results[apt])\n",
    "        bedrooms_sqft_str = get_bedrooms_sqft_str(apt_results[apt])\n",
    "        bedrooms, sqft = get_bedrooms_sqft(bedrooms_sqft_str)\n",
    "        price = get_price(apt_results[apt])\n",
    "        posting_date = get_posting_date(apt_results[apt])\n",
    "        #populate the result dataframe with the characteristics\n",
    "        apts_results_df.loc[apt] = [city, base_URL, href, posting_ID, title, bedrooms, sqft, price, posting_date]\n",
    "\n",
    "    #construct full URL for the listing\n",
    "    apts_results_df['full_URL'] = apts_results_df.apply(lambda row: row['base_URL'] + row['href'], axis = 1)\n",
    "    \n",
    "    #construct reply URL for the listing\n",
    "    apts_results_df['Reply_contact_info_link'] = apts_results_df.apply(lambda row: row['base_URL'] + reply_string + row['posting_ID'].strip('.html'), axis = 1)\n",
    "    \n",
    "    #delete base URL and href columns\n",
    "    del apts_results_df['base_URL']\n",
    "    del apts_results_df['href']\n",
    "    \n",
    "    return apts_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operationalizing Phase  \n",
    "\n",
    "This phase will incorporate the ability to run the scraper across a selection of cities and bedroom and price range specifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## City to Craigslist URLs Dictionary  \n",
    "- If we want to expand the scope of our project to additional cities, this is the place to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create city list\n",
    "cities = ['seattle', 'houston', 'chicago', 'sandiego']\n",
    "\n",
    "#set base craigslist URLs\n",
    "base_URLs = ['https://seattle.craigslist.org', 'https://houston.craigslist.org', 'https://chicago.craigslist.org', \\\n",
    "             'https://sandiego.craigslist.org']\n",
    "\n",
    "#set search URLS to feed to query function\n",
    "search_URLs = ['https://seattle.craigslist.org/search/apa', 'https://houston.craigslist.org/search/apa', \\\n",
    "               'https://chicago.craigslist.org/search/apa', 'https://sandiego.craigslist.org/search/apa']\n",
    "\n",
    "#set reply strings which are intermediate strings between the base URL and the posting ID to access the page where\n",
    "#reply emails are found\n",
    "reply_strings = ['/reply/sea/apa/', '/reply/hou/apa/', '/reply/chi/apa/', '/reply/sdo/apa/']\n",
    "\n",
    "#create dataframe with all of these pieces of information\n",
    "city_to_URL_dict = {'base_URL' : base_URLs, 'search_URL' : search_URLs, 'reply_string' : reply_strings}\n",
    "\n",
    "city_to_URL_df = pd.DataFrame(city_to_URL_dict, index = cities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull data for bedroom, city, price combinations - DO NOT RUN UNLESS YOU ACTUALLY WANT TO PULL NEW LISTINGS \n",
    "\n",
    "- Export results for each city to csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Success. City: seattle, Bedrooms: 1, Price Range: 1000-1500\n",
      "Delay: 13.7592389584\n",
      "Query Success. City: seattle, Bedrooms: 1, Price Range: 1500-2000\n",
      "Delay: 30.7660520077\n",
      "Query Success. City: seattle, Bedrooms: 1, Price Range: 2000-2500\n",
      "Delay: 22.3494670391\n",
      "Query Success. City: seattle, Bedrooms: 1, Price Range: 2500-3000\n",
      "Delay: 36.4256830215\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-08ec6b8fef43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;31m#ping CL server to get query results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mquery_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_search_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminAsk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxAsk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprice\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbedrooms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbedroom\u001b[0m\u001b[0;34m,\u001b[0m                                 \u001b[0msearch_URL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcity_to_URL_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'search_URL'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mquery_results\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-d34395f35f2c>\u001b[0m in \u001b[0;36mfetch_search_results\u001b[0;34m(query, minAsk, maxAsk, bedrooms, search_URL)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msearch_params\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No valid keywords\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msearch_URL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msearch_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# <- no-op if status==200\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nwchen24/anaconda/envs/Machine_learning_python2/lib/python2.7/site-packages/requests/api.pyc\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nwchen24/anaconda/envs/Machine_learning_python2/lib/python2.7/site-packages/requests/api.pyc\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nwchen24/anaconda/envs/Machine_learning_python2/lib/python2.7/site-packages/requests/sessions.pyc\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    486\u001b[0m         }\n\u001b[1;32m    487\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nwchen24/anaconda/envs/Machine_learning_python2/lib/python2.7/site-packages/requests/sessions.pyc\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 641\u001b[0;31m             \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nwchen24/anaconda/envs/Machine_learning_python2/lib/python2.7/site-packages/requests/models.pyc\u001b[0m in \u001b[0;36mcontent\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    779\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 781\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCONTENT_CHUNK_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    782\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_content_consumed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nwchen24/anaconda/envs/Machine_learning_python2/lib/python2.7/site-packages/requests/models.pyc\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'stream'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mProtocolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nwchen24/anaconda/envs/Machine_learning_python2/lib/python2.7/site-packages/requests/packages/urllib3/response.pyc\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    430\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_fp_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecode_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nwchen24/anaconda/envs/Machine_learning_python2/lib/python2.7/site-packages/requests/packages/urllib3/response.pyc\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                 \u001b[0mcache_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Platform-specific: Buggy versions of Python.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m                     \u001b[0;31m# Close the connection when no data is returned\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nwchen24/anaconda/envs/Machine_learning_python2/lib/python2.7/httplib.pyc\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    595\u001b[0m         \u001b[0;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m         \u001b[0;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 597\u001b[0;31m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    598\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mamt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m             \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nwchen24/anaconda/envs/Machine_learning_python2/lib/python2.7/socket.pyc\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    382\u001b[0m                 \u001b[0;31m# fragmentation issues on many platforms.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mEINTR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nwchen24/anaconda/envs/Machine_learning_python2/lib/python2.7/ssl.pyc\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m    764\u001b[0m                     \u001b[0;34m\"non-zero flags not allowed in calls to recv() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m                     self.__class__)\n\u001b[0;32m--> 766\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    767\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nwchen24/anaconda/envs/Machine_learning_python2/lib/python2.7/ssl.pyc\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    651\u001b[0m                 \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 653\u001b[0;31m                 \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    654\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Look at $500 price bucket increments for each number of bedrooms\n",
    "min_prices = np.arange(1000, 4000, 500).tolist()\n",
    "\n",
    "#Look at 1, 2, 3 bedroom apartments\n",
    "bedrooms = [1, 2]\n",
    "\n",
    "#initialize counter which we will use to populate the query result dataframe\n",
    "counter = 0\n",
    "\n",
    "#loop over cities\n",
    "for city in city_to_URL_df.index:\n",
    "    #initialize empty dataframe to hold query results\n",
    "    query_results_df = pd.DataFrame(columns = ('city', 'min_price', 'max_price', 'bedrooms', 'query_html'))\n",
    "\n",
    "    #loop over number of bedrooms\n",
    "    for bedroom in bedrooms:\n",
    "        #loop over the min prices\n",
    "        for price in min_prices:\n",
    "            #set start time\n",
    "            start_time = time.time()\n",
    "\n",
    "            #ping CL server to get query results\n",
    "            query_results, query_encoding = fetch_search_results(query = None, minAsk = price, maxAsk = price + 500, bedrooms = bedroom,\\\n",
    "                                 search_URL = city_to_URL_df.loc[city, 'search_URL'])\n",
    "\n",
    "            if query_results is not None:\n",
    "                print \"Query Success. City: \" + city + \", Bedrooms: \" + str(bedroom) + \", Price Range: \" + str(price) +\\\n",
    "                \"-\" + str(price + 500)            \n",
    "\n",
    "            else:\n",
    "                print \"No Results Found. City: \" + city + \", Bedrooms: \" + str(bedroom) + \", Price Range: \" + str(price) +\\\n",
    "                \"-\" + str(price + 500)            \n",
    "\n",
    "\n",
    "            #append result (which is a string) to query results dataframe\n",
    "            query_results_df.loc[counter] = [city, price, price + 500, bedroom, query_results]\n",
    "\n",
    "            #increment counter\n",
    "            counter += 1\n",
    "\n",
    "            #incorporate delay drawn from normal distribution centered around one minute to hopefully make\n",
    "            #queries appear more human like\n",
    "            delay = abs(np.random.normal(25, 10))\n",
    "            time.sleep(delay)\n",
    "            print \"Delay: \" + str(time.time() - start_time)\n",
    "\n",
    "    #write the results for each city to csv\n",
    "    query_results_df.to_csv(city + '_query_results_output_3-17-17.csv', sep='\\t', encoding='utf-8')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get listing information we need from the query results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#First, read the listings back from csv\n",
    "file_names = []\n",
    "\n",
    "#compile file names to read\n",
    "for city in city_to_URL_df.index:\n",
    "    file_names.append(city + '_query_results_output_3-17-17.csv')\n",
    "\n",
    "#initialize query results df\n",
    "query_results_comb_df = pd.DataFrame(columns = ('Unnamed: 0', 'city', 'min_price', 'max_price', 'bedrooms', 'query_html'))\n",
    "\n",
    "#read each output file for each city and append to a combined dataframe\n",
    "for filename in file_names:\n",
    "    query_results_comb_df = query_results_comb_df.append(pd.read_csv(filename, sep='\\t', encoding='utf-8'))\n",
    "\n",
    "#delete columns we don't want and reset index\n",
    "del query_results_comb_df['Unnamed: 0']\n",
    "query_results_comb_df = query_results_comb_df.reset_index()\n",
    "del query_results_comb_df['index']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop through combined dataframe to get individual listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#initialize listing results df\n",
    "combined_listings_df = pd.DataFrame(columns = ('city', 'posting_ID', 'Listing_Title', 'Bedrooms', 'Sqft', 'Price', 'Posting_Date'))\n",
    "\n",
    "#loop through query results\n",
    "for i in range(query_results_comb_df.shape[0]):\n",
    "    #get city\n",
    "    city_to_use = query_results_comb_df.loc[i, 'city']\n",
    "    #get the intermediate reply string to feed to function that gets individual listings\n",
    "    reply_string_to_use = city_to_URL_df.loc[city_to_use, 'reply_string']\n",
    "    #get base URL to feed to function that gets individual listings\n",
    "    base_URL_to_use = city_to_URL_df.loc[city_to_use, 'base_URL']\n",
    "    #get the query result html code to feed to function that gets individual listings\n",
    "    query_result_html = query_results_comb_df.loc[i, 'query_html']\n",
    "    \n",
    "    #Get individual listings from query results\n",
    "    query_results_df_intermed = compile_listing_URLs(query_result = query_result_html, base_URL = base_URL_to_use,\\\n",
    "                                                     city = city_to_use, reply_string = reply_string_to_use)\n",
    "    \n",
    "    #append the results for each bedroom, city, price combination to the combined dataframe\n",
    "    combined_listings_df = combined_listings_df.append(query_results_df_intermed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Save combined listing dataset to csv\n",
    "#re-order columns\n",
    "combined_listings_df = combined_listings_df[['Listing_Title', 'Posting_Date', 'city', 'posting_ID', 'full_URL', 'Bedrooms',\\\n",
    "                                            'Sqft', 'Price', 'Reply_contact_info_link']]\n",
    "\n",
    "#Add a column where mechanical turk can fill in reply email address\n",
    "combined_listings_df['reply_email_TO_BE_FILLED_IN'] = ''\n",
    "\n",
    "#export to CSV\n",
    "combined_listings_df.to_csv('Combined Scraper Listing Results 3-17-17.csv', sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5760, 10)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_listings_df.head()\n",
    "\n",
    "combined_listings_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(720, 12)\n"
     ]
    }
   ],
   "source": [
    "#Select a random sample from the query results.\n",
    "#Also assign each to treatment group\n",
    "\n",
    "#First, initialize the dataframe to hold the results\n",
    "test_sample_df = pd.DataFrame(columns = ('city', 'Listing_Title', 'Posting_Date', 'Bedrooms', 'Sqft', 'Price',\\\n",
    "                           'posting_ID', 'treatment_assignment', 'full_URL', 'Reply_contact_info_link',\\\n",
    "                                         'reply_email_TO_BE_FILLED_IN', 'Apartment_Complex_Flag'))\n",
    "#create vector of possible treatment assignments\n",
    "#create vector of possible treatment assignments\n",
    "treatments = ['John_Control'] * 15 + ['John_Treat_Low'] * 15 + ['John_Treat_High'] * 15 \\\n",
    "                + ['Jane_Control'] * 15 + ['Jane_Treat_Low'] * 15 + ['Jane_Treat_High'] * 15\n",
    "    \n",
    "for city in cities:\n",
    "    #Instantiate city DF\n",
    "    city_df = pd.DataFrame(columns = ('city', 'Listing_Title', 'Posting_Date', 'Bedrooms', 'Sqft', 'Price',\\\n",
    "                           'posting_ID', 'treatment_assignment', 'full_URL', 'Reply_contact_info_link',\\\n",
    "                                         'reply_email_TO_BE_FILLED_IN', 'Apartment_Complex_Flag'))\n",
    "\n",
    "    for bedroom in [1,2]:\n",
    "        #Take a sample of five listings for each city and bedroom combination\n",
    "        intermed_df = combined_listings_df.loc[(combined_listings_df.city == city)\\\n",
    "                                               & (combined_listings_df.Bedrooms == bedroom),:].sample(n = 90)\n",
    " \n",
    "        #Assign treatment\n",
    "        intermed_df['treatment_assignment'] = intermed_df['treatment_assignment'] =\\\n",
    "            np.random.choice(treatments, replace = False, size = 90)\n",
    "        \n",
    "        #Append results to city df\n",
    "        city_df = city_df.append(intermed_df)\n",
    "        \n",
    "        #shuffle listings within city df\n",
    "        city_df = city_df.sample(frac = 1)\n",
    "        \n",
    "        #re-order columns\n",
    "        city_df = city_df[['city', 'Listing_Title', 'Posting_Date', 'Bedrooms', 'Sqft', 'Price',\\\n",
    "                           'posting_ID', 'treatment_assignment', 'full_URL', 'Reply_contact_info_link',\\\n",
    "                                         'reply_email_TO_BE_FILLED_IN', 'Apartment_Complex_Flag']]\n",
    "\n",
    "        city_df = city_df.sort_values(by = 'treatment_assignment')\n",
    "        \n",
    "        #export each city\n",
    "        city_df.to_csv(city + ' Listings For Data Pull 3-17-17.csv', sep=',', encoding='utf-8')\n",
    "    \n",
    "    #append results to data frame\n",
    "    test_sample_df = test_sample_df.append(city_df)\n",
    "\n",
    "print test_sample_df.shape\n",
    "\n",
    "#test_sample_df.to_csv('Listings For Data Pull 3-17-17.csv', sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomization Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Sqft</th>\n",
       "      <th>Price</th>\n",
       "      <th>Bedrooms</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>city</th>\n",
       "      <th>treatment_assignment</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">chicago</th>\n",
       "      <th>Jane_Control</th>\n",
       "      <td>1027.666667</td>\n",
       "      <td>2136.066667</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jane_Treat_High</th>\n",
       "      <td>1223.444444</td>\n",
       "      <td>2095.433333</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jane_Treat_Low</th>\n",
       "      <td>1032.750000</td>\n",
       "      <td>2055.466667</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>John_Control</th>\n",
       "      <td>989.750000</td>\n",
       "      <td>1790.433333</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>John_Treat_High</th>\n",
       "      <td>1059.333333</td>\n",
       "      <td>2251.400000</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>John_Treat_Low</th>\n",
       "      <td>1051.250000</td>\n",
       "      <td>1859.066667</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">houston</th>\n",
       "      <th>Jane_Control</th>\n",
       "      <td>961.964286</td>\n",
       "      <td>2083.333333</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jane_Treat_High</th>\n",
       "      <td>1078.181818</td>\n",
       "      <td>2373.033333</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jane_Treat_Low</th>\n",
       "      <td>1037.950000</td>\n",
       "      <td>2114.600000</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>John_Control</th>\n",
       "      <td>1005.208333</td>\n",
       "      <td>2153.933333</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>John_Treat_High</th>\n",
       "      <td>1061.782609</td>\n",
       "      <td>2134.400000</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>John_Treat_Low</th>\n",
       "      <td>973.695652</td>\n",
       "      <td>2180.766667</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">sandiego</th>\n",
       "      <th>Jane_Control</th>\n",
       "      <td>870.210526</td>\n",
       "      <td>2045.500000</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jane_Treat_High</th>\n",
       "      <td>893.476190</td>\n",
       "      <td>1872.566667</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jane_Treat_Low</th>\n",
       "      <td>878.304348</td>\n",
       "      <td>1939.633333</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>John_Control</th>\n",
       "      <td>924.913043</td>\n",
       "      <td>2005.633333</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>John_Treat_High</th>\n",
       "      <td>910.851852</td>\n",
       "      <td>2123.100000</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>John_Treat_Low</th>\n",
       "      <td>874.818182</td>\n",
       "      <td>1811.033333</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">seattle</th>\n",
       "      <th>Jane_Control</th>\n",
       "      <td>871.206897</td>\n",
       "      <td>2189.233333</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jane_Treat_High</th>\n",
       "      <td>920.600000</td>\n",
       "      <td>2329.400000</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jane_Treat_Low</th>\n",
       "      <td>875.392857</td>\n",
       "      <td>2155.733333</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>John_Control</th>\n",
       "      <td>908.344828</td>\n",
       "      <td>2373.900000</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>John_Treat_High</th>\n",
       "      <td>873.074074</td>\n",
       "      <td>2134.100000</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>John_Treat_Low</th>\n",
       "      <td>891.413793</td>\n",
       "      <td>2206.733333</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Sqft        Price  Bedrooms\n",
       "city     treatment_assignment                                    \n",
       "chicago  Jane_Control          1027.666667  2136.066667       1.5\n",
       "         Jane_Treat_High       1223.444444  2095.433333       1.5\n",
       "         Jane_Treat_Low        1032.750000  2055.466667       1.5\n",
       "         John_Control           989.750000  1790.433333       1.5\n",
       "         John_Treat_High       1059.333333  2251.400000       1.5\n",
       "         John_Treat_Low        1051.250000  1859.066667       1.5\n",
       "houston  Jane_Control           961.964286  2083.333333       1.5\n",
       "         Jane_Treat_High       1078.181818  2373.033333       1.5\n",
       "         Jane_Treat_Low        1037.950000  2114.600000       1.5\n",
       "         John_Control          1005.208333  2153.933333       1.5\n",
       "         John_Treat_High       1061.782609  2134.400000       1.5\n",
       "         John_Treat_Low         973.695652  2180.766667       1.5\n",
       "sandiego Jane_Control           870.210526  2045.500000       1.5\n",
       "         Jane_Treat_High        893.476190  1872.566667       1.5\n",
       "         Jane_Treat_Low         878.304348  1939.633333       1.5\n",
       "         John_Control           924.913043  2005.633333       1.5\n",
       "         John_Treat_High        910.851852  2123.100000       1.5\n",
       "         John_Treat_Low         874.818182  1811.033333       1.5\n",
       "seattle  Jane_Control           871.206897  2189.233333       1.5\n",
       "         Jane_Treat_High        920.600000  2329.400000       1.5\n",
       "         Jane_Treat_Low         875.392857  2155.733333       1.5\n",
       "         John_Control           908.344828  2373.900000       1.5\n",
       "         John_Treat_High        873.074074  2134.100000       1.5\n",
       "         John_Treat_Low         891.413793  2206.733333       1.5"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Average square footage and price for each city\n",
    "by_city_avgs = test_sample_df.groupby(by = ['city', 'treatment_assignment'])['Sqft', 'Price', 'Bedrooms'].mean()\n",
    "\n",
    "by_city_avgs\n",
    "\n",
    "#combined_listings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price ANOVA p-value: 0.639251057188\n",
      "Square Footage ANOVA p-value: nan\n",
      "Bedroom ANOVA p-value: 1.0\n"
     ]
    }
   ],
   "source": [
    "#Test for statistically significant differences in price, sqft, bedrooms for each treatment\n",
    "import scipy.stats as stats\n",
    "\n",
    "#create groups for price, bedrooms, and squarefootage\n",
    "grps = pd.unique(test_sample_df.treatment_assignment.values)\n",
    "price_data = {grp:test_sample_df['Price'][test_sample_df.treatment_assignment == grp] for grp in grps}\n",
    "sqft_data = {grp:test_sample_df['Sqft'][test_sample_df.treatment_assignment == grp] for grp in grps}\n",
    "bedroom_data = {grp:test_sample_df['Bedrooms'][test_sample_df.treatment_assignment == grp] for grp in grps}\n",
    "\n",
    "\n",
    "\n",
    "#conduct one way ANOVAs\n",
    "\n",
    "#Price\n",
    "F, p = stats.f_oneway(price_data['Jane_Control'], price_data['Jane_Treat_High'], price_data['Jane_Treat_Low'],\\\n",
    "                     price_data['John_Control'], price_data['John_Treat_High'], price_data['John_Treat_Low'])\n",
    "print 'Price ANOVA p-value: ' + str(p)\n",
    "\n",
    "#Sqft\n",
    "F, p = stats.f_oneway(sqft_data['Jane_Control'], sqft_data['Jane_Treat_High'], sqft_data['Jane_Treat_Low'],\\\n",
    "                     sqft_data['John_Control'], sqft_data['John_Treat_High'], sqft_data['John_Treat_Low'])\n",
    "print 'Square Footage ANOVA p-value: ' + str(p)\n",
    "\n",
    "#Bedrooms\n",
    "F, p = stats.f_oneway(bedroom_data['Jane_Control'], bedroom_data['Jane_Treat_High'], bedroom_data['Jane_Treat_Low'],\\\n",
    "                     bedroom_data['John_Control'], bedroom_data['John_Treat_High'], bedroom_data['John_Treat_Low'])\n",
    "print 'Bedroom ANOVA p-value: ' + str(p)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Etract individual groups\n",
    "Jane_Control = test_sample_df[groups[\"Jane_Control\"]]\n",
    "Jane_Treat_High = test_sample_df[groups[\"Jane_Treat_High\"]]\n",
    "Jane_Treat_Low = test_sample_df[groups[\"Jane_Treat_Low\"]]\n",
    "John_Control = test_sample_df[groups[\"John_Control\"]]\n",
    "John_Treat_High = test_sample_df[groups[\"John_Treat_High\"]]\n",
    "John_Treat_Low = test_sample_df[groups[\"John_Treat_Low\"]]\n",
    "\n",
    "# Perform the ANOVA\n",
    "stats.f_oneway(Jane_Control, Jane_Treat_High, Jane_Treat_Low, John_Control, John_Treat_High, John_Treat_Low)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Machine_learning_python2]",
   "language": "python",
   "name": "conda-env-Machine_learning_python2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
